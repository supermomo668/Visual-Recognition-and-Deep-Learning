{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "006b13e2-71ba-4d4b-ba62-53a7c1fd3c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.jit as jit\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "os.environ[\"PYTORCH_JIT\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "113f0c64-af28-41a2-8695-e01fcf24e1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f83787a9-c5b4-4260-a042-b5a0dcedba4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen size:torch.Size([16, 3, 64, 64])\n",
      "Disc size:torch.Size([16, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda\\Anaconda3\\envs\\ptml\\lib\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "gen_x = Generator().cuda()(n_samples=16)\n",
    "print(f\"Gen size:{gen_x.size()}\")\n",
    "print(f\"Disc size:{Discriminator().cuda()(gen_x).size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4faff4e-3236-47f8-993b-286e4b391b66",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebdd704e-c626-42ed-9448-cc34bf12eeab",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from train import *\n",
    "from q1_3 import *\n",
    "from networks import Discriminator, Generator\n",
    "from train import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c205aa1-ae98-4ad3-956a-011971f22c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin\thttps://github.com/visual-learning/generative-modeling (fetch)\n",
      "origin\thttps://github.com/visual-learning/generative-modeling (push)\n"
     ]
    }
   ],
   "source": [
    "!git remote -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db58d954-8ef8-4c13-973a-ed29eec76771",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m3m-m\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Mo\\OneDrive\\Notes\\CMU\\16824 - Visual Learning\\github\\generative-modeling\\gan\\wandb\\run-20221024_105524-l9pg8q43</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/3m-m/vlr-hw2/runs/l9pg8q43\" target=\"_blank\">whole-terrain-6</a></strong> to <a href=\"https://wandb.ai/3m-m/vlr-hw2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File \"C:\\Users\\Mo\\OneDrive\\Notes\\CMU\\16824 - Visual Learning\\github\\generative-modeling\\gan\\networks.py\", line 325, in forward\n        # TODO 1.1: Forward the discriminator assuming a batch of images have been passed in.\n        # Make sure to flatten the output of the convolutional layers and sum across the image dimensions before passing to the output layer!\n        x = self.layers(x)   # (*, 128, 8, 8)\n            ~~~~~~~~~~~ <--- HERE\n        x = torch.sum(x.view(x.size(0), x.size(1),-1), dim=-1)\n        x = self.dense_out(x.view(x.size(0), -1))\n  File \"C:\\ProgramData\\Anaconda\\Anaconda3\\envs\\ptml\\lib\\site-packages\\torch\\nn\\modules\\container.py\", line 141, in forward\n    def forward(self, input):\n        for module in self:\n            input = module(input)\n                    ~~~~~~ <--- HERE\n        return input\n  File \"C:\\Users\\Mo\\OneDrive\\Notes\\CMU\\16824 - Visual Learning\\github\\generative-modeling\\gan\\networks.py\", line 135, in forward\n        shortcut = self.shortcut(x)\n        x = self.layer(x)\n        x = self.residual(x)\n            ~~~~~~~~~~~~~ <--- HERE\n        # Apply self.residual to the output of self.layers and apply self.shortcut to the original input.\n        return torch.add(shortcut, x)\n  File \"C:\\Users\\Mo\\OneDrive\\Notes\\CMU\\16824 - Visual Learning\\github\\generative-modeling\\gan\\networks.py\", line 56, in forward\n        x = x.repeat(1, int(self.downscale_ratio**2), 1, 1)   # (*, C*Up^2, H, W)\n        # 2. Then split channel wise into upscale_factor^2 number of images of shape: (batch x channel x height x width).\n        x = self.downscale_shuffle(x)  # (N, C*Up^2, H/Up, W/Up) = (*, 48, 16, 16)\n            ~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n        # 3. Average the images into one and apply convolution.\n        x = torch.mean(x,dim=1).unsqueeze(1)\n  File \"C:\\ProgramData\\Anaconda\\Anaconda3\\envs\\ptml\\lib\\site-packages\\torch\\nn\\modules\\pixelshuffle.py\", line 103, in forward\n    def forward(self, input: Tensor) -> Tensor:\n        return F.pixel_unshuffle(input, self.downscale_factor)\n               ~~~~~~~~~~~~~~~~~ <--- HERE\nRuntimeError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 4.00 GiB total capacity; 3.02 GiB already allocated; 0 bytes free; 3.09 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5060/187673163.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# TODO 1.3.2: Run this line of code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m train_model(\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mgen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mdisc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Notes\\CMU\\16824 - Visual Learning\\github\\generative-modeling\\gan\\train.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(gen, disc, num_iterations, batch_size, lamb, prefix, gen_loss_fn, disc_loss_fn, log_period, wandb_logging)\u001b[0m\n\u001b[0;32m    107\u001b[0m                 \u001b[1;31m# 3. Compute the discriminator output on the generated data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m                 \u001b[0mgen_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# [N, 3, 64, 64]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m                 \u001b[0mdiscrim_fake\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdisc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m                 \u001b[0mdiscrim_real\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdisc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda\\Anaconda3\\envs\\ptml\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File \"C:\\Users\\Mo\\OneDrive\\Notes\\CMU\\16824 - Visual Learning\\github\\generative-modeling\\gan\\networks.py\", line 325, in forward\n        # TODO 1.1: Forward the discriminator assuming a batch of images have been passed in.\n        # Make sure to flatten the output of the convolutional layers and sum across the image dimensions before passing to the output layer!\n        x = self.layers(x)   # (*, 128, 8, 8)\n            ~~~~~~~~~~~ <--- HERE\n        x = torch.sum(x.view(x.size(0), x.size(1),-1), dim=-1)\n        x = self.dense_out(x.view(x.size(0), -1))\n  File \"C:\\ProgramData\\Anaconda\\Anaconda3\\envs\\ptml\\lib\\site-packages\\torch\\nn\\modules\\container.py\", line 141, in forward\n    def forward(self, input):\n        for module in self:\n            input = module(input)\n                    ~~~~~~ <--- HERE\n        return input\n  File \"C:\\Users\\Mo\\OneDrive\\Notes\\CMU\\16824 - Visual Learning\\github\\generative-modeling\\gan\\networks.py\", line 135, in forward\n        shortcut = self.shortcut(x)\n        x = self.layer(x)\n        x = self.residual(x)\n            ~~~~~~~~~~~~~ <--- HERE\n        # Apply self.residual to the output of self.layers and apply self.shortcut to the original input.\n        return torch.add(shortcut, x)\n  File \"C:\\Users\\Mo\\OneDrive\\Notes\\CMU\\16824 - Visual Learning\\github\\generative-modeling\\gan\\networks.py\", line 56, in forward\n        x = x.repeat(1, int(self.downscale_ratio**2), 1, 1)   # (*, C*Up^2, H, W)\n        # 2. Then split channel wise into upscale_factor^2 number of images of shape: (batch x channel x height x width).\n        x = self.downscale_shuffle(x)  # (N, C*Up^2, H/Up, W/Up) = (*, 48, 16, 16)\n            ~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n        # 3. Average the images into one and apply convolution.\n        x = torch.mean(x,dim=1).unsqueeze(1)\n  File \"C:\\ProgramData\\Anaconda\\Anaconda3\\envs\\ptml\\lib\\site-packages\\torch\\nn\\modules\\pixelshuffle.py\", line 103, in forward\n    def forward(self, input: Tensor) -> Tensor:\n        return F.pixel_unshuffle(input, self.downscale_factor)\n               ~~~~~~~~~~~~~~~~~ <--- HERE\nRuntimeError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 4.00 GiB total capacity; 3.02 GiB already allocated; 0 bytes free; 3.09 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "gen = Generator().cuda().to(memory_format=torch.channels_last)\n",
    "disc = Discriminator().cuda().to(memory_format=torch.channels_last)\n",
    "prefix = \"data_gan/\"\n",
    "os.makedirs(prefix, exist_ok=True)\n",
    "\n",
    "# TODO 1.3.2: Run this line of code.\n",
    "train_model(\n",
    "    gen,\n",
    "    disc,\n",
    "    num_iterations=int(3e4),\n",
    "    batch_size=256,\n",
    "    prefix=prefix,\n",
    "    gen_loss_fn=compute_generator_loss,\n",
    "    disc_loss_fn=compute_discriminator_loss,\n",
    "    log_period=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158d489e-cf0a-4147-89cb-7caa93071edd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ptml]",
   "language": "python",
   "name": "conda-env-ptml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
