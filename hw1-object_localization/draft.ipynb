{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0f052c1-3647-48de-a5c6-2a453165e875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import wandb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from AlexNet import localizer_alexnet, localizer_alexnet_robust\n",
    "from voc_dataset import *\n",
    "from utils import *\n",
    "from task_1 import AverageMeter\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a4e1576-acf5-4f57-96ed-9487b9848a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_WANDB = True  # use flags, wandb is not convenient for debugging\n",
    "model_names = sorted(name for name in models.__dict__\n",
    "                     if name.islower() and not name.startswith(\"__\")\n",
    "                     and callable(models.__dict__[name]))\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch ImageNet Training')\n",
    "parser.add_argument('--arch', default='localizer_alexnet')\n",
    "parser.add_argument(\n",
    "    '-j',\n",
    "    '--workers',\n",
    "    default=2,\n",
    "    type=int,\n",
    "    metavar='N',\n",
    "    help='number of data loading workers (default: 4)')\n",
    "parser.add_argument(\n",
    "    '--epochs',\n",
    "    default=30,\n",
    "    type=int,\n",
    "    metavar='N',\n",
    "    help='number of total epochs to run')\n",
    "parser.add_argument(\n",
    "    '--start-epoch',\n",
    "    default=0,\n",
    "    type=int,\n",
    "    metavar='N',\n",
    "    help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument(\n",
    "    '-b',\n",
    "    '--batch-size',\n",
    "    default=32,\n",
    "    type=int,\n",
    "    metavar='N',\n",
    "    help='mini-batch size (default: 256)')\n",
    "parser.add_argument(\n",
    "    '--lr',\n",
    "    '--learning-rate',\n",
    "    default=1e-2,\n",
    "    type=float,\n",
    "    metavar='LR',\n",
    "    help='initial learning rate')\n",
    "parser.add_argument(\n",
    "    '--momentum', default=0.9, type=float, metavar='M', help='momentum')\n",
    "parser.add_argument(\n",
    "    '--weight-decay',\n",
    "    '--wd',\n",
    "    default=1e-4,\n",
    "    type=float,\n",
    "    metavar='W',\n",
    "    help='weight decay (default: 1e-4)')\n",
    "parser.add_argument(\n",
    "    '--print-freq',\n",
    "    '-p',\n",
    "    default=10,\n",
    "    type=int,\n",
    "    metavar='N',\n",
    "    help='print frequency (default: 10)')\n",
    "parser.add_argument(\n",
    "    '--eval-freq',\n",
    "    default=2,\n",
    "    type=int,\n",
    "    metavar='N',\n",
    "    help='print frequency (default: 10)')\n",
    "parser.add_argument(\n",
    "    '--resume',\n",
    "    default='',\n",
    "    type=str,\n",
    "    metavar='PATH',\n",
    "    help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument(\n",
    "    '-e',\n",
    "    '--evaluate',\n",
    "    dest='evaluate',\n",
    "    action='store_true',\n",
    "    help='evaluate model on validation set')\n",
    "parser.add_argument(\n",
    "    '--pretrained',\n",
    "    dest='pretrained',\n",
    "    action='store_false',\n",
    "    help='use pre-trained model')\n",
    "parser.add_argument(\n",
    "    '--world-size',\n",
    "    default=1,\n",
    "    type=int,\n",
    "    help='number of distributed processes')\n",
    "parser.add_argument(\n",
    "    '--dist-url',\n",
    "    default='tcp://224.66.41.62:23456',\n",
    "    type=str,\n",
    "    help='url used to set up distributed training')\n",
    "parser.add_argument(\n",
    "    '--dist-backend', default='gloo', type=str, help='distributed backend')\n",
    "parser.add_argument('--vis', action='store_true')\n",
    "\n",
    "best_prec1 = 0\n",
    "cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b29ef64c-7459-4032-aaa8-b2fcef77dee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__module__': '__main__', 'parsed_args': Namespace(arch='localizer_alexnet', batch_size=32, dist_backend='gloo', dist_url='tcp://224.66.41.62:23456', epochs=30, eval_freq=2, evaluate=False, lr=0.1, momentum=0.9, pretrained=True, print_freq=10, resume='', start_epoch=0, vis=False, weight_decay=0.0001, workers=2, world_size=1), 'batch_size': 32, 'workers': 2, 'print_freq': 10, 'eval_freq': 2, 'epochs': 30, 'arch': 'localizer_alexnet', 'pretrained': True, 'lr': 0.04, 'momentum': 0.9, 'weight_decay': 0.0001, 'start_epoch': 0, '__dict__': <attribute '__dict__' of 'args' objects>, '__weakref__': <attribute '__weakref__' of 'args' objects>, '__doc__': None}\n"
     ]
    }
   ],
   "source": [
    "class args:\n",
    "    parsed_args = parser.parse_known_args()[0]\n",
    "    batch_size = parsed_args.batch_size\n",
    "    workers = parsed_args.workers\n",
    "    print_freq = parsed_args.print_freq\n",
    "    eval_freq= parsed_args.eval_freq\n",
    "    epochs = parsed_args.epochs\n",
    "    lr= parsed_args.lr\n",
    "    arch = parsed_args.arch\n",
    "    pretrained = True\n",
    "    momentum = parsed_args.momentum\n",
    "    weight_decay = parsed_args.weight_decay\n",
    "    start_epoch = parsed_args.start_epoch\n",
    "print(args.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2de672d2-7dc7-47ab-bbc7-090c91845b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> creating model 'localizer_alexnet'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mo/16824-VLR/hw1-object_localization/AlexNet.py:76: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(layer.weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LocalizerAlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=0, dilation=(1, 1), ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=0, dilation=(1, 1), ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Conv2d(256, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "print(\"=> creating model '{}'\".format(args.arch))\n",
    "if args.arch == 'localizer_alexnet':\n",
    "    model = localizer_alexnet(pretrained=args.pretrained)\n",
    "elif args.arch == 'localizer_alexnet_robust':\n",
    "    model = localizer_alexnet_robust(pretrained=args.pretrained)\n",
    "print(model)\n",
    "\n",
    "model.features = torch.nn.DataParallel(model.features)\n",
    "model.cuda()\n",
    "\n",
    "# TODO (Q1.1): define loss function (criterion) and optimizer from [1]\n",
    "# also use an LR scheduler to decay LR by 10 every 30 epochs\n",
    "criterion = nn.BCELoss().cuda()   #nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=args.lr,\n",
    "                            momentum=args.momentum,\n",
    "                            weight_decay=args.weight_decay)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr,\n",
    "                            weight_decay=args.weight_decay)\n",
    "\n",
    "\"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb3ac4f0-bc80-439b-9a7c-b5a0de840210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 29, 29])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test output size\n",
    "model(torch.randn((3, 512,512)).unsqueeze(dim=0)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbba9287-b68e-4d6f-9c4e-0ed182d3dee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path:/home/mo/16824-VLR/hw1-object_localization/../data/VOCdevkit/VOC2007\n"
     ]
    }
   ],
   "source": [
    "dataset = VOCDataset('trainval', top_n=10, image_size=512, data_dir='../data/VOCdevkit/VOC2007/')\n",
    "# TODO (Q1.1): Create Datasets and Dataloaders using VOCDataset\n",
    "# Ensure that the sizes are 512x512\n",
    "# Also ensure that data directories are correct\n",
    "# The ones use for testing by TAs might be different\n",
    "n = len(dataset)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [int(np.floor(n*0.8)), n-int(np.floor(n*0.8))])\n",
    "train_sampler = torch.utils.data.SubsetRandomSampler(range(len(train_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "514c1e4c-8948-488d-9a9f-b2c428ee761f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m3m-m\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mo/16824-VLR/hw1-object_localization/wandb/run-20221001_202328-1ncoj5ag</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/3m-m/vlr-hw1/runs/1ncoj5ag\" target=\"_blank\">sparkling-cloud-23</a></strong> to <a href=\"https://wandb.ai/3m-m/vlr-hw1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=args.workers,\n",
    "    pin_memory=True,\n",
    "    sampler=train_sampler,\n",
    "    collate_fn=custom_collate_fn_VOC,\n",
    "    drop_last=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=args.workers,\n",
    "    pin_memory=True,\n",
    "    collate_fn=custom_collate_fn_VOC,\n",
    "    drop_last=True)\n",
    "\n",
    "# TODO (Q1.3): Create loggers for wandb.\n",
    "if USE_WANDB:\n",
    "    wandb.init(project=\"vlr-hw1\", reinit=True)\n",
    "#Ideally, use flags since wandb makes it harder to debug code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b205cea-09a4-43b7-88af-d4e01a245fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image False\n",
      "label False\n",
      "wgt False\n",
      "rois True\n",
      "gt_boxes False\n",
      "gt_classes False\n",
      "image <class 'torch.Tensor'>\n",
      "torch.Size([3, 512, 512])\n",
      "label <class 'torch.Tensor'>\n",
      "torch.Size([20])\n",
      "wgt <class 'torch.Tensor'>\n",
      "torch.Size([20])\n",
      "rois <class 'numpy.ndarray'>\n",
      "(10, 4)\n",
      "gt_boxes <class 'list'>\n",
      "(1, 4)\n",
      "gt_classes <class 'list'>\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "data = train_dataset[2020]\n",
    "for k, v in data.items():\n",
    "    print(k, type(v)==np.ndarray)\n",
    "for k, v in data.items():\n",
    "    print(k, type(v))\n",
    "    if type(v) != list: print(v.shape)\n",
    "    else: print(np.array(v).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc0bf36e-a874-498f-b7d9-df1842750291",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def metric1(output, target):\n",
    "    # TODO (Q1.5): compute metric1\n",
    "    target = target.detach().numpy()\n",
    "    output = output.detach().numpy()\n",
    "    mean_ap = sklearn.metrics.average_precision_score(target, output, average='micro')\n",
    "    return mean_ap    #[0]\n",
    "\n",
    "\n",
    "def metric2(output, target, thres=0.5):\n",
    "    # TODO (Q1.5): compute metric2\n",
    "    target = target.detach().numpy()\n",
    "    sig_recall = sklearn.metrics.recall_score(target, (output>thres).detach().numpy(), average='micro')\n",
    "    return sig_recall  #[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e258f63-e513-43d5-829e-0afb8c39b0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: You can add input arguments if you wish\n",
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    avg_m1 = AverageMeter()\n",
    "    avg_m2 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    class_id_to_label = dict(enumerate(dataset.CLASS_NAMES))\n",
    "    end = time.time()\n",
    "    for i, (data) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        # TODO (Q1.1): Get inputs from the data dict\n",
    "        # Convert inputs to cuda if training on GPU\n",
    "        input_im = data['image'].to('cuda')\n",
    "        target_class = data['label']\n",
    "                            \n",
    "        # TODO (Q1.1): Get output from model\n",
    "        if i==0: print(\"Forward pass\")\n",
    "        conv_out = model(input_im)\n",
    "        \n",
    "        # TODO (Q1.1): Perform any necessary functions on the output\n",
    "        imoutput = (nn.MaxPool2d(kernel_size=(conv_out.size(2), conv_out.size(3)))(conv_out)).squeeze()\n",
    "        imoutput = torch.sigmoid(imoutput)\n",
    "         \n",
    "        if i==0: print(f\"Output size:{imoutput.size()}\")\n",
    "        vis_heatmap = F.interpolate(conv_out, size=(input_im.shape[2],input_im.shape[3]), mode='nearest')\n",
    "        if i==0: print(f\"Heatmap output size:{vis_heatmap.shape}\")\n",
    "        \n",
    "        # TODO (Q1.1): Compute loss using ``criterion``\n",
    "        loss = criterion(imoutput.to('cpu'), target_class)\n",
    "        #criterion(data['gt_boxes'], imoutput)\n",
    "        \n",
    "        # measure metrics and record loss\n",
    "        m1 = metric1(imoutput.to('cpu'), target_class)\n",
    "        m2 = metric2(imoutput.to('cpu'), target_class)\n",
    "        losses.update(loss.item(), len(data))\n",
    "        avg_m1.update(m1)\n",
    "        avg_m2.update(m2)\n",
    "\n",
    "        # TODO (Q1.1): compute gradient and perform optimizer step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Metric1 {avg_m1.val:.3f} ({avg_m1.avg:.3f})\\t'\n",
    "                  'Metric2 {avg_m2.val:.3f} ({avg_m2.avg:.3f})'.format(\n",
    "                      epoch,\n",
    "                      i,\n",
    "                      len(train_loader),\n",
    "                      batch_time=batch_time,\n",
    "                      data_time=data_time,\n",
    "                      loss=losses,\n",
    "                      avg_m1=avg_m1,\n",
    "                      avg_m2=avg_m2)\n",
    "                 )\n",
    "        # TODO (Q1.3): Visualize/log things as mentioned in handout at appropriate intervals\n",
    "        c_map = plt.get_cmap('jet')\n",
    "        table = wandb.Table(columns=[\"id\", \"image\", \"heatmap\"])\n",
    "        if (epoch==0 or epoch==1 ) and i==0:\n",
    "            table = wandb.Table(columns=[\"id\", \"image\", \"heatmap\"])\n",
    "            for n, (im, out_heatmap) in enumerate(zip(input_im, vis_heatmap)):\n",
    "                input_img = wandb.Image(im, boxes={\n",
    "                    \"predictions\": {\n",
    "                        \"box_data\": get_box_data(data['gt_classes'][n], data['gt_boxes'][n]),\n",
    "                        \"class_labels\": class_id_to_label,       \n",
    "                    },\n",
    "                })\n",
    "                # Log selective masks\n",
    "                att_map = torch.mean(vis_heatmap[n][data['gt_classes'][n]], dim=0)\n",
    "                table.add_data(n, input_img, wandb.Image(c_map(att_map.cpu().detach().numpy())))\n",
    "                if n==1: break\n",
    "        wandb.log(\n",
    "            {'train/loss':loss, 'train/metric1': m1,  'train/metric2': m2,}\n",
    "        )\n",
    "        # End of train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19bb3820-36a7-4711-802d-c40f01ac57f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, epoch=0):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    avg_m1 = AverageMeter()\n",
    "    avg_m2 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    class_id_to_label = dict(enumerate(dataset.CLASS_NAMES))\n",
    "    end = time.time()\n",
    "    for i, (data) in enumerate(val_loader):\n",
    "        # TODO (Q1.1): Get inputs from the data dict\n",
    "        # Convert inputs to cuda if training on GPU\n",
    "        input_im = data['image'].to('cuda')\n",
    "        target_class = data['label']\n",
    "                            \n",
    "        # TODO (Q1.1): Get output from model\n",
    "        if i==0: print(\"Forward pass\")\n",
    "        conv_out = model(input_im)\n",
    "        \n",
    "        # TODO (Q1.1): Perform any necessary functions on the output\n",
    "        imoutput = (nn.MaxPool2d(kernel_size=(conv_out.size(2), conv_out.size(3)))(conv_out)).squeeze()\n",
    "        imoutput = torch.sigmoid(imoutput)\n",
    "         \n",
    "        if i==0: print(f\"Output size:{imoutput.size()}\")\n",
    "        vis_heatmap = F.interpolate(conv_out, size=(input_im.shape[2],input_im.shape[3]), mode='nearest')\n",
    "        if i==0: print(f\"Heatmap output size:{vis_heatmap.shape}\")\n",
    "        \n",
    "        # TODO (Q1.1): Compute loss using ``criterion``\n",
    "        loss = criterion(imoutput.to('cpu'), target_class)\n",
    "        \n",
    "        # measure metrics and record loss\n",
    "        m1 = metric1(imoutput.to('cpu'), target_class)\n",
    "        m2 = metric2(imoutput.to('cpu'), target_class)\n",
    "        avg_m1.update(m1)\n",
    "        avg_m2.update(m2)\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Metric1 {avg_m1.val:.3f} ({avg_m1.avg:.3f})\\t'\n",
    "                  'Metric2 {avg_m2.val:.3f} ({avg_m2.avg:.3f})'.format(\n",
    "                      i,\n",
    "                      len(val_loader),\n",
    "                      batch_time=batch_time,\n",
    "                      loss=losses,\n",
    "                      avg_m1=avg_m1,\n",
    "                      avg_m2=avg_m2))\n",
    "\n",
    "        # TODO (Q1.3): Visualize things as mentioned in handout\n",
    "        c_map = plt.get_cmap('jet')\n",
    "        \n",
    "        # TODO (Q1.3): Visualize at appropriate intervals\n",
    "        if (epoch==0 or epoch==1 ) and i==0:\n",
    "            table = wandb.Table(columns=[\"id\", \"image\", \"heatmap\"])\n",
    "            for n, (im, out_heatmap) in enumerate(zip(input_im, vis_heatmap)):\n",
    "                input_img = wandb.Image(im, boxes={\n",
    "                    \"predictions\": {\n",
    "                        \"box_data\": get_box_data(data['gt_classes'][n], data['gt_boxes'][n]),\n",
    "                        \"class_labels\": class_id_to_label,       \n",
    "                    },\n",
    "                })\n",
    "                # Log selective masks\n",
    "                att_map = torch.mean(vis_heatmap[n][data['gt_classes'][n]], dim=0)\n",
    "                table.add_data(n, input_img, wandb.Image(c_map(att_map.cpu().detach().numpy())))\n",
    "                if n==1: break\n",
    "        wandb.log(\n",
    "            {'train/loss':loss, 'train/metric1': m1,  'train/metric2': m2,}\n",
    "        )\n",
    "\n",
    "\n",
    "    print(' * Metric1 {avg_m1.avg:.3f} Metric2 {avg_m2.avg:.3f}'.format(\n",
    "        avg_m1=avg_m1, avg_m2=avg_m2))\n",
    "\n",
    "    return avg_m1.avg, avg_m2.avg\n",
    "\n",
    "\n",
    "# TODO: You can make changes to this function if you wish (not necessary)\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca5e401e-a886-44c8-8fc6-dc6d3a581b8c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass\n",
      "Output size:torch.Size([32, 20])\n",
      "Heatmap output size:torch.Size([32, 20, 512, 512])\n",
      "Epoch: [0][0/125]\tTime 2.423 (2.423)\tData 0.850 (0.850)\tLoss 0.6953 (0.6953)\tMetric1 0.099 (0.099)\tMetric2 0.680 (0.680)\n",
      "Epoch: [0][10/125]\tTime 0.114 (0.487)\tData 0.003 (0.270)\tLoss 8.7500 (7.5064)\tMetric1 0.087 (0.083)\tMetric2 0.000 (0.062)\n",
      "Epoch: [0][20/125]\tTime 0.113 (0.431)\tData 0.002 (0.278)\tLoss 7.9688 (7.6150)\tMetric1 0.080 (0.081)\tMetric2 0.000 (0.032)\n",
      "Epoch: [0][30/125]\tTime 0.105 (0.406)\tData 0.004 (0.279)\tLoss 9.2188 (7.8853)\tMetric1 0.149 (0.094)\tMetric2 0.235 (0.063)\n",
      "Epoch: [0][40/125]\tTime 0.105 (0.411)\tData 0.003 (0.294)\tLoss 8.5938 (8.0848)\tMetric1 0.137 (0.112)\tMetric2 0.244 (0.111)\n",
      "Epoch: [0][50/125]\tTime 0.105 (0.402)\tData 0.003 (0.292)\tLoss 7.5000 (8.1123)\tMetric1 0.075 (0.123)\tMetric2 0.000 (0.136)\n",
      "Epoch: [0][60/125]\tTime 0.118 (0.397)\tData 0.013 (0.292)\tLoss 7.8125 (8.0939)\tMetric1 0.078 (0.116)\tMetric2 0.000 (0.114)\n",
      "Epoch: [0][70/125]\tTime 0.105 (0.391)\tData 0.034 (0.290)\tLoss 8.7500 (8.1049)\tMetric1 0.087 (0.111)\tMetric2 0.000 (0.098)\n",
      "Epoch: [0][80/125]\tTime 0.176 (0.387)\tData 0.124 (0.291)\tLoss 7.6562 (8.0148)\tMetric1 0.077 (0.106)\tMetric2 0.000 (0.086)\n",
      "Epoch: [0][90/125]\tTime 0.365 (0.384)\tData 0.308 (0.292)\tLoss 7.1875 (7.9548)\tMetric1 0.072 (0.103)\tMetric2 0.000 (0.076)\n",
      "Epoch: [0][100/125]\tTime 0.429 (0.381)\tData 0.376 (0.293)\tLoss 8.5938 (7.9794)\tMetric1 0.086 (0.101)\tMetric2 0.000 (0.069)\n",
      "Epoch: [0][110/125]\tTime 0.256 (0.378)\tData 0.205 (0.292)\tLoss 8.7500 (7.9812)\tMetric1 0.087 (0.099)\tMetric2 0.000 (0.062)\n",
      "Epoch: [0][120/125]\tTime 0.250 (0.376)\tData 0.197 (0.292)\tLoss 19.6648 (8.2128)\tMetric1 0.084 (0.098)\tMetric2 0.200 (0.063)\n",
      "Forward pass\n",
      "Output size:torch.Size([32, 20])\n",
      "Heatmap output size:torch.Size([32, 20, 512, 512])\n",
      "Test: [0/31]\tTime 0.898 (0.898)\tLoss 0.0000 (0.0000)\tMetric1 0.084 (0.084)\tMetric2 0.231 (0.231)\n",
      "Test: [10/31]\tTime 0.624 (0.430)\tLoss 0.0000 (0.0000)\tMetric1 0.104 (0.086)\tMetric2 0.288 (0.242)\n",
      "Test: [20/31]\tTime 0.597 (0.395)\tLoss 0.0000 (0.0000)\tMetric1 0.078 (0.086)\tMetric2 0.200 (0.253)\n",
      "Test: [30/31]\tTime 0.384 (0.375)\tLoss 0.0000 (0.0000)\tMetric1 0.091 (0.086)\tMetric2 0.294 (0.245)\n",
      " * Metric1 0.086 Metric2 0.245\n",
      "Forward pass\n",
      "Output size:torch.Size([32, 20])\n",
      "Heatmap output size:torch.Size([32, 20, 512, 512])\n",
      "Epoch: [1][0/125]\tTime 0.968 (0.968)\tData 0.866 (0.866)\tLoss 24.8438 (24.8438)\tMetric1 0.088 (0.088)\tMetric2 0.218 (0.218)\n",
      "Epoch: [1][10/125]\tTime 0.243 (0.422)\tData 0.164 (0.339)\tLoss 49.3750 (26.6636)\tMetric1 0.088 (0.080)\tMetric2 0.537 (0.260)\n",
      "Epoch: [1][20/125]\tTime 0.134 (0.480)\tData 0.011 (0.397)\tLoss 61.0938 (40.0083)\tMetric1 0.073 (0.078)\tMetric2 0.434 (0.383)\n",
      "Epoch: [1][30/125]\tTime 0.115 (0.503)\tData 0.010 (0.417)\tLoss 52.3438 (44.6024)\tMetric1 0.062 (0.077)\tMetric2 0.333 (0.416)\n",
      "Epoch: [1][40/125]\tTime 0.099 (0.487)\tData 0.001 (0.401)\tLoss 50.7812 (46.0942)\tMetric1 0.082 (0.077)\tMetric2 0.455 (0.433)\n",
      "Epoch: [1][50/125]\tTime 0.126 (0.465)\tData 0.018 (0.377)\tLoss 50.3125 (47.3686)\tMetric1 0.074 (0.077)\tMetric2 0.479 (0.445)\n",
      "Epoch: [1][60/125]\tTime 0.124 (0.448)\tData 0.003 (0.360)\tLoss 63.1250 (48.9808)\tMetric1 0.068 (0.076)\tMetric2 0.636 (0.458)\n",
      "Epoch: [1][70/125]\tTime 0.116 (0.438)\tData 0.004 (0.349)\tLoss 72.1875 (52.1708)\tMetric1 0.078 (0.076)\tMetric2 0.667 (0.480)\n",
      "Epoch: [1][80/125]\tTime 0.127 (0.430)\tData 0.008 (0.340)\tLoss 78.2812 (54.6516)\tMetric1 0.068 (0.075)\tMetric2 0.608 (0.496)\n",
      "Epoch: [1][90/125]\tTime 0.114 (0.422)\tData 0.001 (0.333)\tLoss 60.6250 (56.3108)\tMetric1 0.064 (0.075)\tMetric2 0.457 (0.505)\n",
      "Epoch: [1][100/125]\tTime 0.111 (0.418)\tData 0.011 (0.328)\tLoss 67.6562 (57.7249)\tMetric1 0.078 (0.075)\tMetric2 0.642 (0.513)\n",
      "Epoch: [1][110/125]\tTime 0.119 (0.412)\tData 0.002 (0.321)\tLoss 54.5312 (58.1760)\tMetric1 0.066 (0.075)\tMetric2 0.535 (0.519)\n",
      "Epoch: [1][120/125]\tTime 0.109 (0.407)\tData 0.006 (0.317)\tLoss 63.5938 (58.9582)\tMetric1 0.073 (0.074)\tMetric2 0.592 (0.519)\n",
      "Forward pass\n",
      "Output size:torch.Size([32, 20])\n",
      "Heatmap output size:torch.Size([32, 20, 512, 512])\n",
      "Epoch: [2][0/125]\tTime 1.084 (1.084)\tData 1.022 (1.022)\tLoss 65.3125 (65.3125)\tMetric1 0.077 (0.077)\tMetric2 0.482 (0.482)\n",
      "Epoch: [2][10/125]\tTime 0.562 (0.416)\tData 0.487 (0.333)\tLoss 69.6875 (67.3530)\tMetric1 0.062 (0.072)\tMetric2 0.522 (0.541)\n",
      "Epoch: [2][20/125]\tTime 0.382 (0.375)\tData 0.309 (0.301)\tLoss 64.0625 (64.2831)\tMetric1 0.069 (0.071)\tMetric2 0.562 (0.525)\n",
      "Epoch: [2][30/125]\tTime 0.250 (0.364)\tData 0.187 (0.294)\tLoss 59.2188 (65.2553)\tMetric1 0.076 (0.071)\tMetric2 0.549 (0.531)\n",
      "Epoch: [2][40/125]\tTime 0.137 (0.358)\tData 0.068 (0.289)\tLoss 69.6875 (64.9606)\tMetric1 0.073 (0.071)\tMetric2 0.519 (0.524)\n",
      "Epoch: [2][50/125]\tTime 0.114 (0.360)\tData 0.001 (0.288)\tLoss 63.9062 (65.5228)\tMetric1 0.069 (0.070)\tMetric2 0.574 (0.528)\n",
      "Epoch: [2][60/125]\tTime 0.112 (0.360)\tData 0.003 (0.285)\tLoss 69.2188 (66.5155)\tMetric1 0.070 (0.070)\tMetric2 0.549 (0.539)\n",
      "Epoch: [2][70/125]\tTime 0.139 (0.360)\tData 0.011 (0.282)\tLoss 75.4688 (66.9799)\tMetric1 0.081 (0.071)\tMetric2 0.774 (0.549)\n",
      "Epoch: [2][80/125]\tTime 0.119 (0.365)\tData 0.001 (0.286)\tLoss 67.1875 (67.7135)\tMetric1 0.077 (0.070)\tMetric2 0.680 (0.555)\n",
      "Epoch: [2][90/125]\tTime 0.122 (0.364)\tData 0.013 (0.284)\tLoss 78.4375 (68.5794)\tMetric1 0.061 (0.070)\tMetric2 0.609 (0.563)\n",
      "Epoch: [2][100/125]\tTime 0.141 (0.364)\tData 0.011 (0.283)\tLoss 76.2500 (69.0202)\tMetric1 0.078 (0.070)\tMetric2 0.722 (0.569)\n",
      "Epoch: [2][110/125]\tTime 0.111 (0.361)\tData 0.004 (0.279)\tLoss 72.9688 (69.4830)\tMetric1 0.072 (0.070)\tMetric2 0.627 (0.576)\n",
      "Epoch: [2][120/125]\tTime 0.124 (0.361)\tData 0.002 (0.278)\tLoss 73.2812 (70.1094)\tMetric1 0.068 (0.070)\tMetric2 0.612 (0.577)\n",
      "Forward pass\n",
      "Output size:torch.Size([32, 20])\n",
      "Heatmap output size:torch.Size([32, 20, 512, 512])\n",
      "Test: [0/31]\tTime 0.931 (0.931)\tLoss 0.0000 (0.0000)\tMetric1 0.076 (0.076)\tMetric2 0.673 (0.673)\n",
      "Test: [10/31]\tTime 0.735 (0.414)\tLoss 0.0000 (0.0000)\tMetric1 0.084 (0.073)\tMetric2 0.644 (0.653)\n",
      "Test: [20/31]\tTime 0.589 (0.376)\tLoss 0.0000 (0.0000)\tMetric1 0.069 (0.073)\tMetric2 0.600 (0.665)\n",
      "Test: [30/31]\tTime 0.308 (0.357)\tLoss 0.0000 (0.0000)\tMetric1 0.074 (0.074)\tMetric2 0.667 (0.652)\n",
      " * Metric1 0.074 Metric2 0.652\n",
      "Forward pass\n",
      "Output size:torch.Size([32, 20])\n",
      "Heatmap output size:torch.Size([32, 20, 512, 512])\n",
      "Epoch: [3][0/125]\tTime 0.920 (0.920)\tData 0.833 (0.833)\tLoss 72.8125 (72.8125)\tMetric1 0.078 (0.078)\tMetric2 0.625 (0.625)\n",
      "Epoch: [3][10/125]\tTime 0.362 (0.394)\tData 0.301 (0.325)\tLoss 63.7500 (70.8665)\tMetric1 0.071 (0.071)\tMetric2 0.583 (0.588)\n",
      "Epoch: [3][20/125]\tTime 0.574 (0.378)\tData 0.498 (0.311)\tLoss 70.1562 (69.0551)\tMetric1 0.068 (0.071)\tMetric2 0.490 (0.560)\n",
      "Epoch: [3][30/125]\tTime 0.531 (0.372)\tData 0.479 (0.304)\tLoss 70.7812 (68.3151)\tMetric1 0.072 (0.071)\tMetric2 0.455 (0.546)\n",
      "Epoch: [3][40/125]\tTime 0.577 (0.369)\tData 0.510 (0.297)\tLoss 64.8438 (68.2540)\tMetric1 0.074 (0.071)\tMetric2 0.509 (0.541)\n",
      "Epoch: [3][50/125]\tTime 0.575 (0.367)\tData 0.510 (0.291)\tLoss 68.5938 (67.8876)\tMetric1 0.067 (0.071)\tMetric2 0.596 (0.546)\n",
      "Epoch: [3][60/125]\tTime 0.592 (0.366)\tData 0.533 (0.288)\tLoss 62.8125 (67.7093)\tMetric1 0.077 (0.072)\tMetric2 0.640 (0.551)\n",
      "Epoch: [3][70/125]\tTime 0.588 (0.364)\tData 0.525 (0.285)\tLoss 63.7500 (67.1714)\tMetric1 0.073 (0.072)\tMetric2 0.580 (0.554)\n",
      "Epoch: [3][80/125]\tTime 0.905 (0.366)\tData 0.838 (0.286)\tLoss 63.2812 (66.7625)\tMetric1 0.068 (0.072)\tMetric2 0.622 (0.557)\n",
      "Epoch: [3][90/125]\tTime 0.502 (0.369)\tData 0.435 (0.287)\tLoss 64.6875 (66.4641)\tMetric1 0.067 (0.071)\tMetric2 0.521 (0.557)\n",
      "Epoch: [3][100/125]\tTime 0.540 (0.366)\tData 0.476 (0.284)\tLoss 69.0625 (66.3454)\tMetric1 0.077 (0.071)\tMetric2 0.554 (0.559)\n",
      "Epoch: [3][110/125]\tTime 0.544 (0.366)\tData 0.491 (0.283)\tLoss 69.0625 (66.5297)\tMetric1 0.085 (0.072)\tMetric2 0.548 (0.563)\n",
      "Epoch: [3][120/125]\tTime 0.648 (0.366)\tData 0.582 (0.284)\tLoss 68.2812 (66.6964)\tMetric1 0.071 (0.072)\tMetric2 0.612 (0.565)\n",
      "Forward pass\n",
      "Output size:torch.Size([32, 20])\n",
      "Heatmap output size:torch.Size([32, 20, 512, 512])\n",
      "Epoch: [4][0/125]\tTime 0.986 (0.986)\tData 0.919 (0.919)\tLoss 68.9062 (68.9062)\tMetric1 0.066 (0.066)\tMetric2 0.574 (0.574)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-17:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/pin_memory.py\", line 28, in _pin_memory_loop\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 113, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/reductions.py\", line 295, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 492, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/connection.py\", line 620, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17875/2569783445.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# evaluate on validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_17875/3920958209.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, criterion, optimizer, epoch)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mclass_id_to_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCLASS_NAMES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;31m# measure data loading time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mdata_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1207\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1208\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1161\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1163\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1164\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "    # train for one epoch\n",
    "    train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    if epoch % args.eval_freq == 0:\n",
    "        m1, m2 = validate(val_loader, model, criterion, epoch)\n",
    "        score = m1 * m2\n",
    "        # remember best prec@1 and save checkpoint\n",
    "        is_best = score > best_prec1\n",
    "        best_prec1 = max(score, best_prec1)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'arch': args.arch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_prec1': best_prec1,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, is_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf400aa0-788c-4903-9ad6-86a3a1aaafd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504a9692-92bf-4f3c-b6d6-f916e33cff1b",
   "metadata": {},
   "source": [
    "<b> Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caa1206-e32a-4252-85fd-f2288b5b1a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import roi_pool, roi_align, RoIPool"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
